# -*- coding: utf-8 -*-
"""example_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1uc7cKXmxL3JbDpQ1aBBFA8iGjCMYNm

TensorFlow 버전 2.x 선택
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf

print(tf.__version__)

import keras

print(keras.__version__)

"""1.   MNIST 데이터 로딩"""

from tensorflow.keras.datasets import mnist

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# 데이터 구조 확인
print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')
print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')

import matplotlib.pyplot as plt

plt.imshow(X_train[0], cmap = 'gray')
plt.show()

"""2.   데이터 전처리"""

# Neural Network에 전송할 때 이미지는 1차원이어야 함.
# 3차원 학습 데이터(samples, height, width)를 
# 2차원 데이터(sampels, height * width) 모양으로 변환함.
n, h, w = X_train.shape
X_train = X_train.reshape(n, h * w)
print('X_train:', X_train.shape)

# X_test 데이터를 (samples, height * width) 모양으로 변환.
nt, ht, wt = X_test.shape
X_test = X_test.reshape(nt, ht * wt)
print('X_test:', X_test.shape)

print(X_train[0])  # 이미지 데이터의 값 확인

"""이미지 데이터는 0 ~ 255 사이의 값들로 저장되어 있음. 신경망에 보낼 때는 정규화된 값(0. ~ 1.)으로 보내면 더 좋은 성능이 나올 수도 있음."""

X_train = X_train.astype('float16') / 255
X_test = X_test.astype('float16') / 255

print(Y_train[0])  # 레이블(클래스)은 one-hot encoding이 아님.

# 학습/테스트 레이블을 one-hot encoding 적용
from tensorflow.keras.utils import to_categorical

Y_train = to_categorical(Y_train, 10, dtype = 'float16')
Y_test = to_categorical(Y_test, 10, dtype = 'float16')
print(Y_train[0], Y_test[0])

"""3.   1층 Neural Network 생성"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Sequential 클래스 인스턴스 생성 - 생성자(constructor) 호출
model = Sequential()  # 신경망 모델

# Dense(fully-connected layer, 완전연결 층)을 생성하고,
# 신경망에 은닉층으로 추가
model.add(Dense(512, input_dim = h * w, activation = 'relu'))
# 512: 은닉층의 뉴런 개수

# 출력층(output layer)을 신경망에 추가 
model.add(Dense(10, activation = 'softmax'))

# 신경망 모델 컴파일
model.compile(loss = 'categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

# 모델 최적화 - epoch를 반복할 때, 10회 epoch 동안 성능 향상이 없으면,
# 자동으로 학습 중단(epoch 중단)
from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor = 'val_loss',
                           patience = 10)

# 신경망 모델 학습 실행
history = model.fit(X_train, Y_train,
                    batch_size = 200,
                    epochs = 50,
                    verbose = 1,
                    callbacks = [early_stop],
                    validation_data = (X_test, Y_test))

# 테스트 정확도 출력
acc = model.evaluate(X_test, Y_test)
# evaluate 메소드는 [loss, accuracy] 리스트를 리턴함.
print('테스트 정확도:', acc[1])

# 각 epoch에서 손실 그래프
# 학습 세트 손실
train_loss = history.history['loss']
# 테스트 세트 손실
test_loss = history.history['val_loss']

# x좌표 - epoch 회수
x = range(len(train_loss))
plt.plot(x, train_loss, label = 'train loss', c = 'red', marker = '.')
plt.plot(x, test_loss, label = 'test loss', c = 'blue', marker = '.')
plt.legend()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
